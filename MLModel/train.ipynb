{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Fine-tuning\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data (name, inpExpOutFunc): \n",
    "    dataset = load_dataset(name)\n",
    "    train_data = dataset['train']\n",
    "    test_data = dataset['validation']\n",
    "\n",
    "    def preprocess_function(dataset):\n",
    "        dataset[\"input_ids\"] = []\n",
    "        dataset[\"attention_mask\"] = []\n",
    "        dataset[\"labels\"] = []\n",
    "        dataset = dict(dataset)\n",
    "        for index in range(len(dataset[list(dataset)[0]])):\n",
    "            inp, exp_out = inpExpOutFunc(dataset, index)\n",
    "            model_inputs = tokenizer(inp, max_length=1024, truncation=True)\n",
    "            labels = tokenizer(exp_out, max_length=1024, truncation=True)\n",
    "            dataset[\"input_ids\"].append(model_inputs[\"input_ids\"])\n",
    "            dataset[\"attention_mask\"].append(model_inputs[\"attention_mask\"])\n",
    "            dataset[\"labels\"].append(labels[\"input_ids\"])\n",
    "        return dataset\n",
    "    \n",
    "    train_data = train_data.map(preprocess_function, batched=True)\n",
    "    test_data = test_data.map(preprocess_function, batched=True)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pytorch_training = False\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# load datasets needed\n",
    "def sciq_extract (dataset, index): \n",
    "    return dataset['support'][index], dataset['question'][index]\n",
    "train_data_sciq, test_data_sciq = load_data(\"sciq\", sciq_extract) # scientific questions and answers\n",
    "\n",
    "def squad_extract (dataset, index):\n",
    "    return dataset['answers'][index][\"text\"][0], dataset['question'][index]\n",
    "train_data_squad, test_data_squad = load_data('squad', squad_extract) # wikipedia questions and answers\n",
    "\n",
    "def piqa_extract (dataset, index): \n",
    "    if dataset[\"label\"][index] == 0:\n",
    "        return dataset[\"sol1\"][index], dataset[\"goal\"][index]\n",
    "    else:\n",
    "        return dataset[\"sol2\"][index], dataset[\"goal\"][index]\n",
    "train_data_piqa, test_data_piqa = load_data(\"piqa\", piqa_extract) # piqa questions and answers, although used for common sense, used questions and answers\n",
    "\n",
    "test_data = datasets.concatenate_datasets((test_data_sciq, test_data_squad, test_data_piqa))\n",
    "train_data = datasets.concatenate_datasets((train_data_sciq, train_data_squad, train_data_piqa))\n",
    "\n",
    "# keep only input_ids, attention_mask, and labels\n",
    "def clean_dataset (dataset): \n",
    "    columns_remove = dataset.column_names\n",
    "    columns_remove.remove(\"input_ids\")\n",
    "    columns_remove.remove(\"attention_mask\")\n",
    "    columns_remove.remove(\"labels\")\n",
    "    return dataset.remove_columns(columns_remove)\n",
    "\n",
    "test_data = clean_dataset(test_data)\n",
    "train_data = clean_dataset(train_data)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "\n",
    "# visualize dataset\n",
    "print(type(train_data)) # <class 'datasets.arrow_dataset.Dataset'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataset to pandas\n",
    "print(train_data.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers' Seq2Seq Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# see https://huggingface.co/docs/transformers/tasks/summarization\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=10,\n",
    "    eval_accumulation_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, randint\n",
    "# load model from checkpoint \n",
    "prompt = \"Natural Language Processing is a field of artificial intelligence in which computers analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation. 'Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,' John Rehling. “By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech to text and automatically translating between languages.” NLP is used to analyze text, allowing machines to understand how humans speak. This human-computer interaction enables real-world applications like automatic text summarization, sentimental analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction, stemming, and more. NLP is characterized as a difficult problem in computer science. Human language is rarely precise, or plainly spoken. To understand human language is to understand not only the words, but the concepts and how they’re linked together to create meaning. Despite language being one of the easiest things for the human mind to learn, the ambiguity of language is what makes natural language processing a difficult problem for computers to master.\"\n",
    "PATH = \"./results/checkpoint-17500\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(PATH, local_files_only=True)\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(f\"{tokenizer.decode(outputs[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv \n",
    "df = pd.read_csv(\"loss.csv\", delimiter=\"\\t\")\n",
    "print(df)\n",
    "# matplotlib the loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(df[\"Training Loss\"])\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9185851d83f890c4daa7c447531b5798471561f3d89a4458e4babb6392f4662e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
